{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb719618-9bbe-4369-b906-5a4f560a506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules\n",
    "#Import ebooklib\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "import re\n",
    "import os\n",
    "#Import nltk\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words, stopwords, names\n",
    "#Import gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "168b3118-7248-439d-ac6d-7c9f92f81b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPUB files directory\n",
    "epub_folder = \".\\epubs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3070ce67-2ccc-4afa-a554-e722e2409f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limit Paragraph\n",
    "def merge_strings_until_limit(strings, min_length, max_length, test_for_max = 0):\n",
    "    merged_string = \"\"\n",
    "    merged_strings = []\n",
    "    \n",
    "    for s in strings:\n",
    "        if len(merged_string) <= min_length:\n",
    "            merged_string += s\n",
    "        \n",
    "        elif len(merged_string) > max_length and test_for_max<5:\n",
    "                splitParagraph = merged_string.split('.')\n",
    "                splitParagraphRePoint = []\n",
    "                for sp in splitParagraph:\n",
    "                    splitParagraphRePoint.append(sp+'.')\n",
    "                \n",
    "                merged = merge_strings_until_limit(splitParagraphRePoint, min_length, max_length, test_for_max+1)\n",
    "                merged_strings.extend(merged)\n",
    "                merged_string = s\n",
    "        else:\n",
    "            merged_strings.append(merged_string)\n",
    "            merged_string = s\n",
    "    \n",
    "    if merged_string:\n",
    "        merged_strings.append(merged_string)\n",
    "    \n",
    "    return merged_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11ff5b4-8c2f-4124-8c37-bbf1b293f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read .epub files in Paragraphs\n",
    "def read_epub_paragraphs(epub_file, epub_ID):\n",
    "    book = epub.read_epub(epub_file)\n",
    "    paragraphs = []\n",
    "    \n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "        content = item.get_content().decode('utf-8')\n",
    "        content = re.sub('<[^<]+?>', '', content)\n",
    "        content = re.sub('\\s+', ' ', content)\n",
    "        content = re.sub('\\n', ' ', content)\n",
    "        \n",
    "        paragraphs.extend(content.strip().split(\"&#13;\"))\n",
    "    \n",
    "    paragraphs = merge_strings_until_limit(paragraphs, 200, 1000)\n",
    "    paragraphs = [{'TEXT':paragraphs[i], 'NR':i, 'Book ID':epub_ID} for i in range(len(paragraphs))]\n",
    "    \n",
    "    return paragraphs[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75b8946-bc6a-4c1b-9e28-508f7dcfa81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\envs\\urbandesignenv\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "C:\\Users\\Patrick\\anaconda3\\envs\\urbandesignenv\\Lib\\site-packages\\ebooklib\\epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'TEXT': ' It also recognizes that all this activity is not founded on a solid intellectual basis that might empower its practitioners to have the critical courage to resist demands to simply supply more and more excitement to a market ravenous for spectacle and entertainment.',\n",
       "  'NR': 11,\n",
       "  'Book ID': 2}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "paragraphs = read_epub_paragraphs(\"epubs\\A Philosophy of Curating.epub\", 2)\n",
    "paragraphs[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de4d7a7-9247-425d-842e-4c6846c7bff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TEXT': ' He raised his Sunday Guardian newspaper to block my view. He shooed me away, telling me to find some book to read or work to do. At times it seemed as if Papa was on the brink of remembering. I imagined pulling the word off his tongue if only I knew the first syllable.',\n",
       "  'NR': 11,\n",
       "  'Book ID': 1}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "paragraphs = read_epub_paragraphs(\"epubs\\A Map to the Door of No Return - Dionne Brand.epub\", 1)\n",
    "paragraphs[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328471f7-87a8-4e91-aeaa-6468bfa0195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download from nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e91ae24-eed5-41ba-9bc4-fee125cd05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Words\n",
    "ENGLISH_WORDS = set(words.words())\n",
    "\n",
    "def is_english_word(word):\n",
    "    return (word.lower() in ENGLISH_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60926437-4f2a-4e42-b0b5-5bc3c3f1b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizer, Stop Words, Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def processed_documents(words): \n",
    "    #Lemmatize\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    #Stop Words\n",
    "    filtered_words = [word for word in lemmatized_words if ((word not in STOP_WORDS) and is_english_word(word))]\n",
    "    #Stemmer\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    #Join\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4f6d42-6ac3-4ddb-ac44-5ce1a2a67c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_epub_files(epub_folder):\n",
    "    output_folder = \".\\processed_texts\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for epub_file in os.listdir(epub_folder):\n",
    "        if epub_file.endswith('.epub'):\n",
    "            epub_path = os.path.join(epub_folder, epub_file)\n",
    "            epub_ID = os.path.splitext(epub_file)[0]\n",
    "            paragraphs = read_epub_paragraphs(epub_path, epub_ID)    \n",
    "            print(f\"Processing {epub_file}...\")\n",
    "\n",
    "            with open(os.path.join(output_folder, f\"{epub_ID}_processed.txt\"), 'w', encoding='utf-8') as output_file:\n",
    "                for paragraph in paragraphs:\n",
    "                    words = gensim.utils.simple_preprocess(paragraph['TEXT'], min_len = 3, deacc=True)\n",
    "                    processed_text = processed_documents(words)\n",
    "                    output_file.write(f\"TEXT: {paragraph['TEXT']}, NR: {paragraph['NR']}, Book ID: {epub_ID}, Processed Text: {processed_text}\\n\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0f50a4-1db7-475e-9b1b-8e86f9257d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing A Map to the Door of No Return - Dionne Brand.epub...\n",
      "Processing A Philosophy of Curating.epub...\n",
      "Processing About Looking by John Berger.epub...\n",
      "Processing Anne Sexton Poems.epub...\n",
      "Processing Architecture and the Body by Kim Sexton.epub...\n",
      "Processing Migration Crises and the Struct.epub...\n",
      "Processing Society After Money Dialogue.epub...\n",
      "Processing Speculative Everything by Anthony Dunne.epub...\n",
      "Processing The Age of Gold - H. W. Brands.epub...\n",
      "Processing The Poetry of Architecture by John Ruskin.epub...\n"
     ]
    }
   ],
   "source": [
    "process_epub_files(epub_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4019f0ea-8561-42f6-bfee-2efe5b94587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11a05406-6720-4d52-8afd-10ce778e5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle(epub_folder):\n",
    "    output_folder = \".\\pickle_file\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for epub_file in os.listdir(epub_folder):\n",
    "        if epub_file.endswith('.epub'):\n",
    "            epub_path = os.path.join(epub_folder, epub_file)\n",
    "            epub_ID = os.path.splitext(epub_file)[0]\n",
    "            paragraphs = read_epub_paragraphs(epub_path, epub_ID)\n",
    "            print(f\"Saving {epub_file} to pickle...\")\n",
    "\n",
    "            processed_data = []\n",
    "            for paragraph in paragraphs:\n",
    "                    words = gensim.utils.simple_preprocess(paragraph['TEXT'], min_len = 3, deacc=True)\n",
    "                    processed_text = processed_documents(words)\n",
    "                    processed_data.append({'TEXT': paragraph['TEXT'], 'NR': paragraph['NR'], 'Book ID': epub_ID, 'Processed Text': processed_text})\n",
    "\n",
    "            with open(os.path.join(output_folder, f\"{epub_ID}_processed.pkl\"), 'wb') as output_file:\n",
    "                        pickle.dump(processed_data, output_file)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c32fcfb-2023-43ae-a93a-35b58b1804f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving A Map to the Door of No Return - Dionne Brand.epub to pickle...\n",
      "Saving A Philosophy of Curating.epub to pickle...\n",
      "Saving About Looking by John Berger.epub to pickle...\n",
      "Saving Anne Sexton Poems.epub to pickle...\n",
      "Saving Architecture and the Body by Kim Sexton.epub to pickle...\n",
      "Saving Migration Crises and the Struct.epub to pickle...\n",
      "Saving Society After Money Dialogue.epub to pickle...\n",
      "Saving Speculative Everything by Anthony Dunne.epub to pickle...\n",
      "Saving The Age of Gold - H. W. Brands.epub to pickle...\n",
      "Saving The Poetry of Architecture by John Ruskin.epub to pickle...\n"
     ]
    }
   ],
   "source": [
    "#Saving to Pickle\n",
    "save_to_pickle(epub_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e7081-315b-4f16-9e84-925b82010a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
