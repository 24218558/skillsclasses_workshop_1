{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccaae74-3a0f-4053-907b-fed51d439604",
   "metadata": {},
   "source": [
    "__Paragraph Searching__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9d66e-2edb-435a-8404-faddb23b98f4",
   "metadata": {},
   "source": [
    "### ebooks (.epub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769efc77-1b7b-442c-8019-9b96b6f123f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d2f8b10-a1c7-4baf-aa5c-b9f44134af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a14c3853-205b-4635-a6ba-cab8ea50f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epub_book(epub_file, epub_ID): #epub_file is the epub name and epub_ID is the epub number\n",
    "    book = epub.read_epub(epub_file)\n",
    "    text = \"\"\n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "                content = item.get_content().decode('utf-8')\n",
    "                content = re.sub(r'<[^<]+?>', '', content)\n",
    "                content = re.sub(r'\\s+', ' ', content)\n",
    "                content = re.sub(r'\\n', ' ', content)\n",
    "                content = re.sub(r'&#13;', ' ', content)\n",
    "                text += content\n",
    "    book_item = {\"Text\":text, \"ID\":epub_ID}\n",
    "    return book_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e2205ca-2417-464a-93da-5e2a350ffa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\envs\\urbandesignenv\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "C:\\Users\\Patrick\\anaconda3\\envs\\urbandesignenv\\Lib\\site-packages\\ebooklib\\epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "book = read_epub_book(\"epubs\\A Philosophy of Curating.epub\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8ff310f-49f7-4331-8061-f9ba19c484f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8928c27-ec1a-40d6-ac1d-94b39d13c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epub_sentences(epub_file, epub_ID):\n",
    "    book = epub.read_epub(epub_file)\n",
    "    sentences = []\n",
    "    \n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "        content = item.get_content().decode('utf-8')\n",
    "        content = re.sub('<[^<]+?>', '', content)\n",
    "        content = re.sub('\\s+', ' ', content)\n",
    "        content = re.sub('\\n', ' ', content)\n",
    "        content = re.sub('&#13;', ' ', content)\n",
    "        \n",
    "        sentences.extend(content.strip().split(\".\"))\n",
    "    \n",
    "    sentences = [{'Paragraph':sentences[i], 'Number':i, 'Book ID':epub_ID} for i in range(len(sentences1))]\n",
    "    \n",
    "    return sentences[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c21df9ec-7223-4f24-8668-06dc68f2c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_epub_sentences(\"epubs\\A Philosophy of Curating.epub\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f137f9e-69ca-44d0-888e-a0cefdf8ee71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Paragraph': ' We wish to talk about curating, because we thought we saw a possibility nestling within its protocols, a possibility for other ways of working, relating and knowing',\n",
       "  'Number': 11,\n",
       "  'Book ID': 2},\n",
       " {'Paragraph': ' This wish of ours to talk about curating led us to institute a space of gathering, a practice-led PhD programme in 2006 called Curatorial/Knowledge to which many young curators and artists have come to share in the discussion and embark on their own investigations',\n",
       "  'Number': 12,\n",
       "  'Book ID': 2}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bd666fd-d23f-405e-b82a-3ca166a19466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_strings_until_limit(strings, min_length, max_length, test_for_max = 0):\n",
    "    merged_string = \"\"\n",
    "    merged_strings = []\n",
    "    \n",
    "    for s in strings:\n",
    "        if len(merged_string) <= min_length:\n",
    "            merged_string += s\n",
    "        \n",
    "        elif len(merged_string) > max_length and test_for_max<5:\n",
    "                splitParagraph = merged_string.split('.')\n",
    "                splitParagraphRePoint = []\n",
    "                for sp in splitParagraph:\n",
    "                    splitParagraphRePoint.append(sp+'.')\n",
    "                \n",
    "                merged = merge_strings_until_limit(splitParagraphRePoint, min_length, max_length, test_for_max+1)\n",
    "                merged_strings.extend(merged)\n",
    "                merged_string = s\n",
    "        else:\n",
    "            merged_strings.append(merged_string)\n",
    "            merged_string = s\n",
    "    \n",
    "    if merged_string:\n",
    "        merged_strings.append(merged_string)\n",
    "    \n",
    "    return merged_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ccd19357-a54e-49b0-b8e4-985a30ad0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epub_paragraphs(epub_file, epub_ID):\n",
    "    book = epub.read_epub(epub_file)\n",
    "    paragraphs = []\n",
    "    \n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "        content = item.get_content().decode('utf-8')\n",
    "        content = re.sub('<[^<]+?>', '', content)\n",
    "        content = re.sub('\\s+', ' ', content)\n",
    "        content = re.sub('\\n', ' ', content)\n",
    "        \n",
    "        paragraphs.extend(content.strip().split(\"&#13;\"))\n",
    "    \n",
    "    paragraphs = merge_strings_until_limit(paragraphs, 200, 1000)\n",
    "    paragraphs = [{'Paragraph':paragraphs[i], 'Number':i, 'Book ID':epub_ID} for i in range(len(paragraphs1))]\n",
    "    \n",
    "    return paragraphs[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d8e39fc-9f7e-42ef-afd5-4476c7d6de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = read_epub_paragraphs(\"epubs\\A Philosophy of Curating.epub\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9661c84-8206-4f36-a3dd-a0b346a662bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Paragraph': ' It also recognizes that all this activity is not founded on a solid intellectual basis that might empower its practitioners to have the critical courage to resist demands to simply supply more and more excitement to a market ravenous for spectacle and entertainment.',\n",
       "  'Number': 11,\n",
       "  'Book ID': 2},\n",
       " {'Paragraph': ' It has always been our desire to enter the discussion as ‘provocateurs’ rather than as ‘experts’ – we have understood that historical and other expertise is easily converted into the legitimation of market-driven spectacles and therefore cannot provide the self-reflexive speculation we continue to think the field requires if it is to become more than a series of professional protocols.',\n",
       "  'Number': 12,\n",
       "  'Book ID': 2},\n",
       " {'Paragraph': ' Alongside these market-driven spectacles a whole gamut of curatorial activities take place, calling into question what it is that is really taking place underneath all this glitter. These activities have taken many shapes: for example, we have seen the entry of the pedagogical into the field under the aegis of ‘the educational turn’, the (re)animation of abandoned sites and the (re-)infiltration of existing institutions, and we have also witnessed a strong insistence on talking, conversing, discussing and reading, activities that are in themselves often understood as the very stuff of what it is to make things visible, legible and relevant.',\n",
       "  'Number': 13,\n",
       "  'Book ID': 2},\n",
       " {'Paragraph': ' And so our discussions have taken place between these two quite opposite poles of what it means to work in the field, two poles whose differences have become increasingly accentuated – bowing to the expanding market on the one hand and an ever-increasing activist spirit within sectors of the worlds of art and artistic education.',\n",
       "  'Number': 14,\n",
       "  'Book ID': 2},\n",
       " {'Paragraph': ' Initially we recognized a necessity to distinguish between ‘curating’ and ‘the curatorial’. If ‘curating’ is a gamut of professional practices that had to do with setting up exhibitions and other modes of display, then ‘the curatorial’ operates at a very different level: it explores all that takes place on the stage set-up, both intentionally and unintentionally, by the curator and views it as an event of knowledge.',\n",
       "  'Number': 15,\n",
       "  'Book ID': 2},\n",
       " {'Paragraph': ' So to drive home a distinction between ‘curating’ and ‘the curatorial’ means to emphasize a shift from the staging of the event to the actual event itself: its enactment, dramatization and performance.',\n",
       "  'Number': 16,\n",
       "  'Book ID': 2}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[10:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5aff455b-4ba6-4fa6-8a5b-cef7deb2262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk Module\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words, stopwords, names\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d72e3-e6dd-456d-81aa-07b4a04f3446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be0f10e1-99db-49a2-8d30-ceca8089a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_WORDS = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d6272ac-c836-4632-ad53-3f68df21df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(word):\n",
    "    return (word.lower() in ENGLISH_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f8e54cf-7020-4574-9f49-1018a6741457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec26839e-deb2-4ea4-a428-b5544002c707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Initially we recognized a necessity to distinguish between ‘curating’ and ‘the curatorial’. If ‘curating’ is a gamut of professional practices that had to do with setting up exhibitions and other modes of display, then ‘the curatorial’ operates at a very different level: it explores all that takes place on the stage set-up, both intentionally and unintentionally, by the curator and views it as an event of knowledge.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[14][\"Paragraph\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5db410c1-7813-45a9-beec-a0a8e259eabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['initially', 'recognized', 'necessity', 'distinguish', 'between', 'curating', 'and', 'the', 'curatorial', 'curating', 'gamut', 'professional', 'practices', 'that', 'had', 'with', 'setting', 'exhibitions', 'and', 'other', 'modes', 'display', 'then', 'the', 'curatorial', 'operates', 'very', 'different', 'level', 'explores', 'all', 'that', 'takes', 'place', 'the', 'stage', 'set', 'both', 'intentionally', 'and', 'unintentionally', 'the', 'curator', 'and', 'views', 'event', 'knowledge']\n"
     ]
    }
   ],
   "source": [
    "paragraph = paragraphs[14][\"Paragraph\"]\n",
    "words = gensim.utils.simple_preprocess(paragraph, min_len = 3, deacc=True)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dea7a71b-56e9-44f8-9084-cb552d1529c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['initially', 'recognized', 'necessity', 'distinguish', 'between', 'curating', 'and', 'the', 'curatorial', 'curating', 'gamut', 'professional', 'practice', 'that', 'had', 'with', 'setting', 'exhibition', 'and', 'other', 'mode', 'display', 'then', 'the', 'curatorial', 'operates', 'very', 'different', 'level', 'explores', 'all', 'that', 'take', 'place', 'the', 'stage', 'set', 'both', 'intentionally', 'and', 'unintentionally', 'the', 'curator', 'and', 'view', 'event', 'knowledge']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b6f7411-1854-4432-b478-6d6c86b55922",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5833e8d2-424e-4d7e-891b-ee7776c1f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [word for word in lemmatized_words if ((word not in STOP_WORDS) and is_english_word(word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac76b3fd-e311-42b1-8d28-1484ce274287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['initially',\n",
       " 'necessity',\n",
       " 'distinguish',\n",
       " 'curatorial',\n",
       " 'gamut',\n",
       " 'professional',\n",
       " 'practice',\n",
       " 'setting',\n",
       " 'exhibition',\n",
       " 'mode',\n",
       " 'display',\n",
       " 'curatorial',\n",
       " 'different',\n",
       " 'level',\n",
       " 'take',\n",
       " 'place',\n",
       " 'stage',\n",
       " 'set',\n",
       " 'intentionally',\n",
       " 'unintentionally',\n",
       " 'curator',\n",
       " 'view',\n",
       " 'event',\n",
       " 'knowledge']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5cbe88ef-2e29-408b-b3ba-7306584722f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['initi', 'necess', 'distinguish', 'curatori', 'gamut', 'profession', 'practic', 'set', 'exhibit', 'mode', 'display', 'curatori', 'differ', 'level', 'take', 'place', 'stage', 'set', 'intent', 'unintent', 'curat', 'view', 'event', 'knowledg']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef64fc56-576b-4724-b649-619c2f1da21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initi necess distinguish curatori gamut profession practic set exhibit mode display curatori differ level take place stage set intent unintent curat view event knowledg\n"
     ]
    }
   ],
   "source": [
    "processed_doc = \" \".join(stemmed_words)\n",
    "print(processed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece306e6-63c8-4df8-88c8-c561339497cb",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2debf21d-c0e6-4908-ad10-6e86b636d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/urbandesignpython/lib/python3.12/site-packages/ebooklib/epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "/opt/homebrew/anaconda3/envs/urbandesignpython/lib/python3.12/site-packages/ebooklib/epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "#Initialize text variable\n",
    "text = \"\"\n",
    "\n",
    "#Read each book\n",
    "for epub_name in epub_list:\n",
    "    epub_path = os.path.join(epub_dir, epub_name)\n",
    "    try:\n",
    "        book = epub.read_epub(epub_path)\n",
    "        for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "            content = item.get_content().decode('utf-8')\n",
    "            content = re.sub(r'<[^<]+?>', '', content)\n",
    "            content = re.sub(r'\\s+', ' ', content)\n",
    "            content = re.sub(r'\\n', ' ', content)\n",
    "            content = re.sub(r'&#13;', ' ', content)\n",
    "            text += content\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6834db47-9108-4f2b-97d3-156a6221a9be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bookPaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m book_paragraphs \u001b[38;5;241m=\u001b[39m [read_epub_paragraphs(b, i) \u001b[38;5;28;01mfor\u001b[39;00m b, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mbookPaths\u001b[49m, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(bookPaths)))]\n\u001b[1;32m      2\u001b[0m all_paragraphs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m book_paragraphs:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bookPaths' is not defined"
     ]
    }
   ],
   "source": [
    "book_paragraphs = [read_epub_paragraphs(b, i) for b, i in zip(bookPaths, range(len(bookPaths)))]\n",
    "all_paragraphs = []\n",
    "for s in book_paragraphs:\n",
    "    all_paragraphs.extend(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "095f0889-c66f-43a0-9fce-7f2307da4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = read_epub_paragraphs('epubs/Roald Dahl_4.epub', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "384fee92-95a4-4c66-906a-03b68e05573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = paragraphs[25]['paragraph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d96f739b-bfc1-45a4-8457-7ca2e28704b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' So you can see that because there were no great pleasures while living in the desert, the small pleasures became great pleasures and the pleasures of children became the pleasures of grown men. That was true for everyone; for the pilots, the fitters, the riggers, the corporals who cooked the food and the men who kept the stores. It was true for the Stag and for Stuffy, so true that when the two of them wangled a forty-eight-hour pass and a lift by air into Cairo, and when they got to the hotel, they were feeling about having a bath rather as you would feel on the first night of your honeymoon.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12497a42-31bd-440d-9149-32ae4e94e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'can', 'see', 'that', 'because', 'there', 'were', 'great', 'pleasures', 'while', 'living', 'the', 'desert', 'the', 'small', 'pleasures', 'became', 'great', 'pleasures', 'and', 'the', 'pleasures', 'children', 'became', 'the', 'pleasures', 'grown', 'men', 'that', 'was', 'true', 'for', 'everyone', 'for', 'the', 'pilots', 'the', 'fitters', 'the', 'riggers', 'the', 'corporals', 'who', 'cooked', 'the', 'food', 'and', 'the', 'men', 'who', 'kept', 'the', 'stores', 'was', 'true', 'for', 'the', 'stag', 'and', 'for', 'stuffy', 'true', 'that', 'when', 'the', 'two', 'them', 'wangled', 'forty', 'eight', 'hour', 'pass', 'and', 'lift', 'air', 'into', 'cairo', 'and', 'when', 'they', 'got', 'the', 'hotel', 'they', 'were', 'feeling', 'about', 'having', 'bath', 'rather', 'you', 'would', 'feel', 'the', 'first', 'night', 'your', 'honeymoon']\n"
     ]
    }
   ],
   "source": [
    "words = gensim.utils.simple_preprocess(paragraph, min_len = 3, deacc=True)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4cc3693-57ac-416b-9061-c367946df8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'can', 'see', 'that', 'because', 'there', 'were', 'great', 'pleasure', 'while', 'living', 'the', 'desert', 'the', 'small', 'pleasure', 'became', 'great', 'pleasure', 'and', 'the', 'pleasure', 'child', 'became', 'the', 'pleasure', 'grown', 'men', 'that', 'wa', 'true', 'for', 'everyone', 'for', 'the', 'pilot', 'the', 'fitter', 'the', 'rigger', 'the', 'corporal', 'who', 'cooked', 'the', 'food', 'and', 'the', 'men', 'who', 'kept', 'the', 'store', 'wa', 'true', 'for', 'the', 'stag', 'and', 'for', 'stuffy', 'true', 'that', 'when', 'the', 'two', 'them', 'wangled', 'forty', 'eight', 'hour', 'pas', 'and', 'lift', 'air', 'into', 'cairo', 'and', 'when', 'they', 'got', 'the', 'hotel', 'they', 'were', 'feeling', 'about', 'having', 'bath', 'rather', 'you', 'would', 'feel', 'the', 'first', 'night', 'your', 'honeymoon']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da80789c-3f67-4ac0-8809-371a6d427965",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1238371-1512-4a03-a733-db06d0c6d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [word for word in lemmatized_words if ((word not in STOP_WORDS) and is_english_word(word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b42e4c23-06b4-47b5-a452-b0fc62613a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['see', 'great', 'pleasur', 'live', 'desert', 'small', 'pleasur', 'great', 'pleasur', 'pleasur', 'child', 'pleasur', 'grown', 'men', 'wa', 'true', 'everyon', 'pilot', 'fitter', 'rigger', 'corpor', 'food', 'men', 'kept', 'store', 'wa', 'true', 'stag', 'stuffi', 'true', 'two', 'forti', 'eight', 'hour', 'lift', 'air', 'got', 'hotel', 'feel', 'bath', 'rather', 'would', 'feel', 'first', 'night', 'honeymoon']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bca50fd-2f8a-40cf-8a63-0e9b4e664bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see great pleasur live desert small pleasur great pleasur pleasur child pleasur grown men wa true everyon pilot fitter rigger corpor food men kept store wa true stag stuffi true two forti eight hour lift air got hotel feel bath rather would feel first night honeymoon\n"
     ]
    }
   ],
   "source": [
    "processed_doc = \" \".join(stemmed_words)\n",
    "print(processed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25d9b6-2f02-4df8-8f4f-edd213404355",
   "metadata": {},
   "source": [
    "## _For homework, combine the above to a function that can process a list of paragraphs and return a list of processed paragraphs (hint: only assign the stop words, lemmatizer and stemmer once)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d573122e-7a9e-46f9-b101-4876ef8132d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def preprocess(paragraphs):\\n    ...'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def preprocess(paragraphs):\n",
    "    ...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d021ad3-f754-45f5-b54a-84f6764c8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import preprocess # you have to define this function yourself, so remove this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "174402ee-c5e4-4bfb-a7ed-9f0dc664c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = preprocess(p['paragraph'] for p in paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "263cc008-38a2-4e87-afad-6cf9f659dc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sever wound join fighter squadron later saw servic fighter pilot went assist air attach start write transfer intellig end war wing command',\n",
       " 'first twelv short stori base wartim experi origin publish lead magazin afterward book highli acclaim stori wide translat becom bestsel world',\n",
       " 'televis dramat select short stori titl tale unexpect among public two volum autobiographi boy go solo much prais novel uncl book ghost stori editor',\n",
       " 'last year life compil book anecdot recip wife felic publish penguin cookbook one success well known child writer book read child world',\n",
       " 'includ giant peach charli chocol factori magic finger charli great glass elev fantast fox twit witch winner award',\n",
       " '',\n",
       " 'die time describ one wide read influenti writer gener wrote obituari child love stori made favourit classic futur vote nation favourit author world book day poll',\n",
       " 'inform best penguin book publish penguin book ltd strand inc street new new book ltd road book ltd avenu penguin book ltd commun centr park new book ltd airborn road new book south ltd avenu south',\n",
       " 'penguin book ltd regist offic strand penguin com select first publish great ltd publish penguin book copyright',\n",
       " 'copyright right reserv except unit state book sold subject condit shall way trade otherwis lent sold hire otherwis circul without publish prior consent form bind cover publish without similar condit includ condit impos subsequ purchas']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9bd27-5ef8-45ad-9754-5b3a5f2f46cf",
   "metadata": {},
   "source": [
    "### Now turn these into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f63a61ef-4c15-4ec7-987b-b33ff902c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fe07955-ed2a-4f17-94e7-45c93cc6ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f0defb6-fd51-4a71-8eb8-230be09e49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecf7cfb4-e8d8-4572-b9d2-a32e8c9cd932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2469x3810 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 64665 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43ae9741-e600-4eac-bda1-c1ba86221639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2469"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1e12b9d-6aeb-4fd2-a6dc-5c2ca4102181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abid', 'abil', 'abl', ..., 'young', 'younger', 'youth'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0354c20d-a5fa-43a1-9a4f-f2e37e73b16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3810"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c597b1-9ccf-4107-b963-9b27cd59670a",
   "metadata": {},
   "source": [
    "### Now search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "923041a0-103d-4db0-bff5-72610b157b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1548ebe9-5dec-4e21-91f4-5c5fd418559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop cultur\n"
     ]
    }
   ],
   "source": [
    "query = 'shopping for culture'\n",
    "processedQuery = preprocess([query])[0]\n",
    "print(processedQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "551393ce-36c2-42ca-9509-deb6e22d9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = vectorizer.transform([processedQuery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eef85eec-00b7-46f5-a593-85849f21938f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3810 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33f13591-0df1-4c77-947d-546285509c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(query_vector, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7cf6dd05-35d6-4957-893b-230cdffb906a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee2da76c-7673-4251-9ca4-f11af9a16b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbor_index = similarities.argmax()\n",
    "nearest_neighbor = tfidf_matrix[nearest_neighbor_index]\n",
    "similarity_score = similarities[0, nearest_neighbor_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0da1fee5-64b7-4f48-b995-e52fa7b157f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbor_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b09f7c8-9541-46a4-9491-2bd73a55c595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph': ' It comes not from any one thing or from any one place; it comes from everything everywhere; from the gutters and the sidewalks, from the houses and the shops and the things in the shops and the food cooking in the shops, from the horses and the dung of the horses in the streets and from the drains; it comes from the people and the way the sun bears down upon the people and from the way the sun bears down upon the gutters and the drains and the horses and the food and the refuse in the streets.',\n",
       " 'nr': 38,\n",
       " 'bookID': 4}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[nearest_neighbor_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "660b006f-83af-4908-b895-a39af512c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indices = nearest_neighbor.toarray().flatten().argsort()[::-1]\n",
    "top_terms = [vectorizer.get_feature_names_out()[idx] for idx in top_indices[:10]]  # Top 10 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dacf02a1-307f-4dcf-853a-7071d83bad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hors',\n",
       " 'shop',\n",
       " 'gutter',\n",
       " 'drain',\n",
       " 'bear',\n",
       " 'sun',\n",
       " 'food',\n",
       " 'street',\n",
       " 'come',\n",
       " 'upon']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_terms\n",
    "# note: show the nature of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9e51b-181d-4a9f-8c22-53751677aff4",
   "metadata": {},
   "source": [
    "### Reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5783fb1-85fe-4a19-9099-a0e171e07126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import random\n",
    "from sklearn.decomposition import TruncatedSVD # also known as Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1408bfd8-8d3a-41a1-9e4f-6ba57d973098",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "svd = TruncatedSVD(n_components=n_components, algorithm = 'randomized')\n",
    "reduced_matrix = svd.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3822e55-114c-4bc7-b5c9-89ef7049153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_query_vec = svd.transform(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d93f8d4-0eda-4415-a0d9-a0531fc499b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities2 = cosine_similarity(reduced_query_vec, reduced_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c54e6d08-e5e1-474d-ae4f-52924142cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbor_index = similarities2.argmax()\n",
    "nearest_neighbor = tfidf_matrix[nearest_neighbor_index]\n",
    "similarity_score = similarities2[0, nearest_neighbor_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7f3ec43-3fb4-467c-af51-2ed155663167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1224"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbor_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6aaf1177-6b4e-41d7-9d70-6aa7512b6bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph': ' Soon the taxi pulled up outside a shop that had three brass balls hanging over the entrance. ‘Wait for me, please,’ Mrs Bixby said to the driver, and she got out of the taxi and entered the shop. There was an enormous cat crouching on the counter eating fishheads out of a white saucer. The animal looked up at Mrs Bixby with bright yellow eyes, then looked away again and went on eating. Mrs Bixby stood by the counter, as far away from the cat as possible, waiting for someone to come, staring at the watches, the shoe buckles, the enamel brooches, the old binoculars, the broken spectacles, the false teeth. Why did they always pawn their teeth, she wondered.',\n",
       " 'nr': 1225,\n",
       " 'bookID': 4}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[nearest_neighbor_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d36b0f88-6a77-45fe-94d0-53bc5c017a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has to do with the concept of Eigenvectors - read Eigen Architecture. \n",
    "reconstructed_tfidf_vector = np.dot(reduced_matrix[nearest_neighbor_index], svd.components_)\n",
    "top_indices = reconstructed_tfidf_vector.argsort()[::-1]\n",
    "top_terms = [vectorizer.get_feature_names_out()[idx] for idx in top_indices[:10]]  # Top 10 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e405ce6c-951c-4db4-b19a-433ee161bfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'wait', 'away', 'look', 'pleas', 'man', 'went', 'eye', 'got', 'minut']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_terms\n",
    "# note: show the nature of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96480e21-cdf0-4730-ba40-58b0efed98ea",
   "metadata": {},
   "source": [
    "# Vectorise Text - 2 Only Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebcfe94e-1db2-4c5c-a1b2-756272a425ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f246b21-1482-4ad2-ba49-b8103d24c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence['paragraph'].lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d929eecc-5e36-4c1f-a383-d42801515f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=3, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3dba24f5-483c-4db0-a3c3-2868e7502e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for culture :\n",
      " [-0.5543372   1.0849485   0.07277424 -1.3508224  -0.25442946  1.1446602\n",
      "  0.12633209 -0.23562469  0.02031419 -0.6486235  -0.02537878 -0.6821772\n",
      "  0.0180017   0.5895747   0.43006945  0.7380041   1.0274788  -1.4864498\n",
      " -0.5329047  -0.3187426  -0.22155413  0.2997045   0.06058976 -0.38840207\n",
      "  0.60665     2.2722745  -0.60194    -0.4182849  -0.31238222 -0.20840423\n",
      "  0.31731045  0.5747491   0.54479414  0.94029516 -0.61047393  0.4619989\n",
      "  0.52466226  0.84420264 -0.12788455 -0.44592467  0.81982505 -0.15400907\n",
      "  0.4750442   0.02426784  0.8506945   0.38597223 -0.32196873  1.2257816\n",
      "  0.10587588  0.02067195]\n",
      "Words similar to culture: [('racial', 0.9058100581169128), ('arts', 0.894705593585968), ('cultural', 0.8944821357727051)]\n"
     ]
    }
   ],
   "source": [
    "# Example: get vector for a specific word\n",
    "word = 'culture'\n",
    "word_vector = w2v_model.wv[word]\n",
    "print(f\"Vector for {word} :\\n\", word_vector)\n",
    "\n",
    "similar_words = w2v_model.wv.most_similar(word, topn=3)\n",
    "print(f\"Words similar to {word}:\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fc5e0-b701-4d8c-a77b-ba1c9cff8808",
   "metadata": {},
   "source": [
    "__Store and load objects:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9cbc99bb-d6a0-4665-8829-8a381d193143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('tokenized_sentences.pickle', 'wb') as gerkin:\n",
    "#    pickle.dump(tokenized_sentences, gerkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2eb066b-2001-4a59-8203-1091bdc6c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a larger set:\n",
    "with open('tokenized_sentences.pickle', 'rb') as gerkin:\n",
    "    tokenized_sentences = pickle.load(gerkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7decad4-ffae-4971-a45f-ee08d49bd1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=3, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9235a-f9c6-47a3-a628-7169fffed58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: get vector for a specific word\n",
    "word = 'culture'\n",
    "word_vector = w2v_model.wv[word]\n",
    "print(f\"Vector for {word} :\\n\", word_vector)\n",
    "\n",
    "similar_words = w2v_model.wv.most_similar(word, topn=3)\n",
    "print(f\"Words similar to {word}:\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bbc8c2-dd0f-44b0-a2cd-0979d56b2392",
   "metadata": {},
   "source": [
    "# Vectorise Text - 3 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "051070f5-fe62-4cac-878f-a64f5ff99a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25a4629-37d9-46e0-a476-be63851c5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "model = FlagModel('BAAI/bge-large-en-v1.5',\n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13426b2d-1259-4d8d-a756-a1a99d3d0b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|█████████████████████| 10/10 [01:26<00:00,  8.70s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings1 = model.encode([p['paragraph'] for p in paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf4fe444-9792-4ae6-9d5c-d9e4473f3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = model.encode_queries(['shopping for culture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63ee7c9b-50b1-47c6-951a-d727c93fcbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = embeddings1 @ queries.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2191ff75-d0ba-44ca-9446-538ee1874e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_index = similarities.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3991d376-1759-4950-a8ba-34ca75fd94e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6237045-8b17-4205-9cad-0b24bfa6c94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph': ' Mike Schofield was an amiable, middle-aged man. But he was a stockbroker. To be precise, he was a jobber in the stock market, and like a number of his kind, he seemed to be somewhat embarrassed, almost ashamed to find that he had made so much money with so slight a talent. In his heart he knew that he was not really much more than a bookmaker – an unctuous, infinitely respectable, secretly unscrupulous bookmaker – and he knew that his friends knew it, too. So he was seeking now to become a man of culture, to cultivate a literary and aesthetic taste, to collect paintings, music, books, and all the rest of it. His little sermon about Rhine wine and Moselle was a part of this thing, this culture that he sought.',\n",
       " 'nr': 330,\n",
       " 'bookID': 4}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[top_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb109e-4832-4da5-9706-1a1dda6e806c",
   "metadata": {},
   "source": [
    "### Variation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9ccb2c43-0986-4ce1-8ee0-f07bcb2a52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7f15a674-c31d-4132-a61a-254c5fe32fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "model2 = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True) # trust_remote_code is needed to use the encode method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "84eba3ef-3adc-4917-98b4-ad697c062072",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings2 = model2.encode([p['paragraph'] for p in paragraphs])\n",
    "query2 = model2.encode(['shopping for culture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "93172c3b-a8dc-402b-bd8a-d36ce3675750",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities2 = cos_sim(embeddings2, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7e288be2-969a-44d4-a0d8-30677f42eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_index2 = similarities2.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "efb9f1ae-ccb2-4956-94dc-c39ebb78fa46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1713"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4499d9a0-5a7d-4e3e-8cde-c6b1cc0e2c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph': ' Two new visitors came in and sat down – a middle-aged husband and a middle-aged wife, the wife carrying a wicker shopping-basket containing groceries. ‘Next, please,’ said the guide, and the woman with the long white gloves got up and left.',\n",
       " 'nr': 1714,\n",
       " 'bookID': 4}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[top_index2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225f2f7-e7e5-4b8e-b3b6-30b46850a6ec",
   "metadata": {},
   "source": [
    "## Text and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ded10-41af-4de1-a442-8a6d91b93c31",
   "metadata": {},
   "source": [
    "### PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc6a3ab6-e2c5-495e-bb74-55c01931c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import fitz # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1b90915-91e2-45fb-92a3-d4eaac4d0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = \"Koolhaas__Elements_of_Architecture.pdf\"\n",
    "pdf_document = fitz.open(pdf_file_path)\n",
    "page = pdf_document.load_page(9)\n",
    "textInfo = page.get_text('blocks',flags=1+2+8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06704f22-e71a-4693-aa0e-15f6c7c1b646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32.150001525878906,\n",
       "  19.075035095214844,\n",
       "  142.0625,\n",
       "  29.402536392211914,\n",
       "  'PAGE 4\\nFLOOR\\n',\n",
       "  0,\n",
       "  0),\n",
       " (33.349998474121094,\n",
       "  528.4000244140625,\n",
       "  484.10699462890625,\n",
       "  577.864013671875,\n",
       "  'negotiating between gravity and\\n',\n",
       "  1,\n",
       "  0),\n",
       " (31.700000762939453,\n",
       "  580.1380615234375,\n",
       "  114.8379898071289,\n",
       "  594.6805419921875,\n",
       "  'keller easterling\\n',\n",
       "  2,\n",
       "  0),\n",
       " (31.200000762939453,\n",
       "  605.6375122070312,\n",
       "  527.1849975585938,\n",
       "  693.2505493164062,\n",
       "  'The floor is the customary technology for negotiating between gravity and the upright body. Every \\nstep is magnetized to its surface. It is the architectural element that is almost always touching \\nthe body. Floors can fall away or out from under, but they are usually there, idling beneath us. Throughout \\nmost of its history, the floor has been a basic assumption, often a starting point. Occasionally, it \\nhas been a place to display elaborate patterns and graphic story telling, or a surface reflecting \\nmathematical fascinations like tessellation. But for eons, the floor was simply the surface of the earth \\nor a technical, architectural response to make that surface more habitable or useful. It has also \\nsometimes been a muted registration of cultural practices and construction technologies.\\n',\n",
       "  3,\n",
       "  0),\n",
       " (31.450000762939453,\n",
       "  495.112548828125,\n",
       "  351.6144714355469,\n",
       "  512.1934814453125,\n",
       "  'Early 19th century Guide to prostration: Akstafa prayer rug from Azerbaijan, featuring niche pattern, \\na reference to the mihrab in every mosque, which points towards Mecca, with stylized hand prints.\\n',\n",
       "  4,\n",
       "  0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2c97e35-b03b-4c85-b9b0-c0e86135dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [t[4] for t in textInfo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1330bbd-7b9e-4f5b-a774-a65bb299f85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAGE 4\\nFLOOR\\n',\n",
       " 'negotiating between gravity and\\n',\n",
       " 'keller easterling\\n',\n",
       " 'The floor is the customary technology for negotiating between gravity and the upright body. Every \\nstep is magnetized to its surface. It is the architectural element that is almost always touching \\nthe body. Floors can fall away or out from under, but they are usually there, idling beneath us. Throughout \\nmost of its history, the floor has been a basic assumption, often a starting point. Occasionally, it \\nhas been a place to display elaborate patterns and graphic story telling, or a surface reflecting \\nmathematical fascinations like tessellation. But for eons, the floor was simply the surface of the earth \\nor a technical, architectural response to make that surface more habitable or useful. It has also \\nsometimes been a muted registration of cultural practices and construction technologies.\\n',\n",
       " 'Early 19th century Guide to prostration: Akstafa prayer rug from Azerbaijan, featuring niche pattern, \\na reference to the mihrab in every mosque, which points towards Mecca, with stylized hand prints.\\n']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c12fd160-d8e8-443c-9f2f-fe647eda2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = {}\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        textInfo = page.get_text(\"blocks\", flags=1+2+8)\n",
    "        paragraphs = [re.sub('\\n', ' ', t[4]) for t in textInfo]\n",
    "        text[page_num] = paragraphs\n",
    "    pdf_document.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66625fd1-43b1-4611-b519-84d4743746b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = \"Koolhaas__Elements_of_Architecture.pdf\"\n",
    "output_text = extract_text_from_pdf(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13f2e04b-818b-48e6-a768-e3a9629e3fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAGE 4 FLOOR ',\n",
       " 'negotiating between gravity and ',\n",
       " 'keller easterling ',\n",
       " 'The floor is the customary technology for negotiating between gravity and the upright body. Every  step is magnetized to its surface. It is the architectural element that is almost always touching  the body. Floors can fall away or out from under, but they are usually there, idling beneath us. Throughout  most of its history, the floor has been a basic assumption, often a starting point. Occasionally, it  has been a place to display elaborate patterns and graphic story telling, or a surface reflecting  mathematical fascinations like tessellation. But for eons, the floor was simply the surface of the earth  or a technical, architectural response to make that surface more habitable or useful. It has also  sometimes been a muted registration of cultural practices and construction technologies. ',\n",
       " 'Early 19th century Guide to prostration: Akstafa prayer rug from Azerbaijan, featuring niche pattern,  a reference to the mihrab in every mosque, which points towards Mecca, with stylized hand prints. ']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4df7c45d-3169-4a6f-807c-c188f76e125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path, image_output_dir):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    images = []\n",
    "\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        image_list = page.get_images(full=True)\n",
    "\n",
    "        for img_index, img_info in enumerate(image_list):\n",
    "            image = pdf_document.extract_image(img_info[0])\n",
    "            image_path = f\"{image_output_dir}/page_{page_num + 1}_image_{img_index}.png\"\n",
    "            with open(image_path, \"wb\") as image_file:\n",
    "                image_file.write(image[\"image\"])\n",
    "\n",
    "            images.append(image_path)\n",
    "\n",
    "    pdf_document.close()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "434bdaa4-ba69-4301-a759-d96dbf30b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_dir = pdf_file_path.split('.')[0] + '_Images'\n",
    "os.mkdir(output_image_dir)\n",
    "extracted_images = extract_images_from_pdf(pdf_file_path, output_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a3a6a-f9ed-43b9-af0b-34e56be07373",
   "metadata": {},
   "source": [
    "# Homework\n",
    "- Multiple books\n",
    "- Make functions for each separate method\n",
    "- Create a combining function that lets you input any sentence and get an answer back\n",
    "- Compare the results of the different methods\n",
    "- Add option to specify how many answers you get back.\n",
    "\n",
    "Suggestions\n",
    "- Add functionality that you can specify which method it's using\n",
    "- Test which method works best for recommending an entire book\n",
    "- Make it so that it also works for a set of pdfs\n",
    "- Create the ability to search for images based on context\n",
    "- Search text by image based on context\n",
    "- Same but now based on fuyu or clip\n",
    "\n",
    "_Challenge:_\n",
    "- read Miro's Play among books and implement his word vectorisation based on books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f7bf4-84ea-484a-a45a-4f230a29422c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
